{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание по занятию \"Рекуррентные сети 3\""
      ],
      "metadata": {
        "id": "HN4V9xQXIjVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Возьмите англо-русскую пару фраз (www.manythings.org....org/anki/)\n",
        "2. Обучите на них seq2seq по аналогии с занятием. Оцените полученное качество\n",
        "3. Попробуйте добавить +1 рекуррентный слой в encoder и decoder\n",
        "4. Попробуйте заменить GRU ячейки на lstm-ячейки\n",
        "5. Оцените качество во всех случаях\n"
      ],
      "metadata": {
        "id": "AWgLA5G6Im1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### импорт библиотек"
      ],
      "metadata": {
        "id": "xfE-jkIlIuLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "yAvAC-d0IgNR",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:56:08.188801Z",
          "iopub.execute_input": "2023-09-02T14:56:08.189240Z",
          "iopub.status.idle": "2023-09-02T14:56:12.134106Z",
          "shell.execute_reply.started": "2023-09-02T14:56:08.189209Z",
          "shell.execute_reply": "2023-09-02T14:56:12.132963Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  1.Загрузка и подготовка данных"
      ],
      "metadata": {
        "id": "mVHWYkZkI825"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://download.pytorch.org/tutorial/data.zip\n",
        "!wget https://www.manythings.org/anki/rus-eng.zip\n",
        "!unzip rus-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5wpvY_BJACh",
        "outputId": "c6756a59-ceb6-4f05-bee2-3e218664508d",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:56:17.613243Z",
          "iopub.execute_input": "2023-09-02T14:56:17.614016Z",
          "iopub.status.idle": "2023-09-02T14:56:21.134758Z",
          "shell.execute_reply.started": "2023-09-02T14:56:17.613979Z",
          "shell.execute_reply": "2023-09-02T14:56:21.133593Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2023-09-02 14:56:18--  https://www.manythings.org/anki/rus-eng.zip\nResolving www.manythings.org (www.manythings.org)... 173.254.30.110\nConnecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 15824155 (15M) [application/zip]\nSaving to: ‘rus-eng.zip’\n\nrus-eng.zip         100%[===================>]  15.09M  28.0MB/s    in 0.5s    \n\n2023-09-02 14:56:19 (28.0 MB/s) - ‘rus-eng.zip’ saved [15824155/15824155]\n\nArchive:  rus-eng.zip\n  inflating: rus.txt                 \n  inflating: _about.txt              \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv '/kaggle/working/rus.txt' '/kaggle/working/eng-rus.txt'"
      ],
      "metadata": {
        "id": "kxV6dFNIiqHT",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:57:23.726352Z",
          "iopub.execute_input": "2023-09-02T14:57:23.727328Z",
          "iopub.status.idle": "2023-09-02T14:57:24.667168Z",
          "shell.execute_reply.started": "2023-09-02T14:57:23.727290Z",
          "shell.execute_reply": "2023-09-02T14:57:24.665858Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail /kaggle/working/eng-rus.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPzqiBiZjsLI",
        "outputId": "9bd248d8-e1c4-4377-905f-a2e674d71ac2",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:57:31.052742Z",
          "iopub.execute_input": "2023-09-02T14:57:31.053118Z",
          "iopub.status.idle": "2023-09-02T14:57:32.007069Z",
          "shell.execute_reply.started": "2023-09-02T14:57:31.053084Z",
          "shell.execute_reply": "2023-09-02T14:57:32.005716Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "We need to uphold laws against discrimination — in hiring, and in housing, and in education, and in the criminal justice system. That is what our Constitution and our highest ideals require.\tНам нужно отстаивать законы против дискриминации при найме на работу, в жилищной сфере, в сфере образования и правоохранительной системе. Этого требуют наша Конституция и высшие идеалы.\tCC-BY 2.0 (France) Attribution: tatoeba.org #5762728 (BHO) & #6390439 (odexed)\nI've heard that you should never date anyone who is less than half your age plus seven. Tom is now 30 years old and Mary is 17. How many years will Tom need to wait until he can start dating Mary?\tЯ слышал, что никогда не следует встречаться с кем-то вдвое младше вас плюс семь лет. Тому 30 лет, a Мэри 17. Сколько лет Тому нужно ждать до тех пор, пока он сможет начать встречаться с Мэри?\tCC-BY 2.0 (France) Attribution: tatoeba.org #10068197 (CK) & #10644473 (notenoughsun)\nI do have one final ask of you as your president, the same thing I asked when you took a chance on me eight years ago. I'm asking you to believe, not in my ability to bring about change but in yours.\tУ меня же, как у вашего президента, есть к вам последняя просьба. Та же самая, что и восемь лет назад, когда вы оказали мне своё доверие. Я прошу вас верить, но не в мои способности добиться перемен, а в ваши.\tCC-BY 2.0 (France) Attribution: tatoeba.org #5762723 (BHO) & #6390123 (odexed)\nIn today's world, we have to equip all our kids with an education that prepares them for success, regardless of what they look like, or how much their parents make, or the zip code that they live in.\tВ современном мире перед нами стоит задача дать всем нашим детям такое образование, которое настроит их на успех вне зависимости от того, как они выглядят, сколько зарабатывают их родители или какой у них почтовый индекс.\tCC-BY 2.0 (France) Attribution: tatoeba.org #3924477 (BHO) & #5968115 (odexed)\nDeath is something that we're often discouraged to talk about or even think about, but I've realized that preparing for death is one of the most empowering things you can do. Thinking about death clarifies your life.\tСмерть - это зачастую то, разговоры или даже мысли о чем приводят в уныние, но я осознал, что готовность умереть наделяет силой, как ничто другое. Мысль о смерти вносит ясность в твою жизнь.\tCC-BY 2.0 (France) Attribution: tatoeba.org #1969892 (davearms) & #3231553 (kukla)\nAt a moment when our economy is growing, our businesses are creating jobs at the fastest pace since the 1990s, and wages are starting to rise again, we have to make some choices about the kind of country we want to be.\tВ тот момент, когда наша экономика растёт, наши предприятия создают рабочие места наибольшими темпами, начиная с 90-х годов, а зарплаты снова начинают расти, мы должны принять ряд решений относительно того, какой страной мы хотим быть.\tCC-BY 2.0 (France) Attribution: tatoeba.org #3924474 (BHO) & #4509418 (odexed)\nWhen I was younger, I hated going to weddings. My grandmothers and aunts would huddle around me, poke me in the side, and giggle \"You're next! You're next!\" They only stopped this nonsense when I began to do the same thing at funerals.\tКогда я была помоложе, я ненавидела ходить на свадьбы. Мои бабушки и тётки толпились вокруг, тыкали меня в бок и говорили, посмеиваясь: «Ты следующая! Ты следующая!». Они перестали нести этот вздор только тогда, когда я начала делать то же самое на похоронах.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2776770 (AlanF_US) & #4311406 (odexed)\nSince there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\tПоскольку сайтов, посвящённых какой-либо теме, как правило, несколько, я обычно просто нажимаю на кнопку \"назад\", если попадаю на страницу со всплывающей рекламой. Я просто перехожу на следующую страницу, найденную гуглом, и надеюсь найти что-то менее раздражающее.\tCC-BY 2.0 (France) Attribution: tatoeba.org #954270 (CK) & #6383010 (odexed)\nIf someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\tЕсли кто-то незнакомый говорит, что вы говорите как носитель языка, это значит, что он, вероятно, заметил что-то в вашей речи, что дало ему понять, что вы не носитель. Другими словами, вы не говорите как носитель.\tCC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #10644468 (notenoughsun)\nDoubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.\tНесомненно, для каждого мужчины в этом мире где-то есть подходящая женщина, которая может стать ему женой, обратное верно и для женщин. Но если учесть, что у человека может быть максимум несколько сотен знакомых, из которых лишь дюжина, а то и меньше, тех, кого он знает близко, а из этой дюжины у него один или от силы два друга, то можно легко увидеть, что с учётом миллионов живущих на Земле людей, ни один подходящий мужчина, возможно, ещё не встретил подходящую женщину.\tCC-BY 2.0 (France) Attribution: tatoeba.org #7697649 (RM) & #7730831 (odexed)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### фукции для подготовки данных"
      ],
      "metadata": {
        "id": "wbeaA5ryOiXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sep = 'CC-BY'\n",
        "with open('/kaggle/working/eng-rus.txt') as file:\n",
        "    for line in file:\n",
        "        print(line.split(sep, 1)[0])\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce1ChYxgRiPM",
        "outputId": "bcfec037-6e4a-4124-a05c-7c92721d087d",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:57:40.928868Z",
          "iopub.execute_input": "2023-09-02T14:57:40.929259Z",
          "iopub.status.idle": "2023-09-02T14:57:40.936190Z",
          "shell.execute_reply.started": "2023-09-02T14:57:40.929224Z",
          "shell.execute_reply": "2023-09-02T14:57:40.935231Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Go.\tМарш!\t\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словаря\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "01O9yrnuOfbc",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:57:46.555141Z",
          "iopub.execute_input": "2023-09-02T14:57:46.555536Z",
          "iopub.status.idle": "2023-09-02T14:57:46.564086Z",
          "shell.execute_reply.started": "2023-09-02T14:57:46.555501Z",
          "shell.execute_reply": "2023-09-02T14:57:46.563029Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# нормализация текста\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?а-яА-Я]+\", r\" \", s) # оставляем русс и англ буквы, точку и воскл. знак\n",
        "    return s"
      ],
      "metadata": {
        "id": "FXKs8j4bM9t6",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:57:49.081218Z",
          "iopub.execute_input": "2023-09-02T14:57:49.081918Z",
          "iopub.status.idle": "2023-09-02T14:57:49.088432Z",
          "shell.execute_reply.started": "2023-09-02T14:57:49.081881Z",
          "shell.execute_reply": "2023-09-02T14:57:49.087464Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  чтение словаря\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    lines = [i.split('\\tCC-BY', 1)[0] for i in lines] # отрезаем все, что после \\tCC-BY\n",
        "\n",
        "        # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "D8T4VxZeM9t-",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:57:55.834718Z",
          "iopub.execute_input": "2023-09-02T14:57:55.835089Z",
          "iopub.status.idle": "2023-09-02T14:57:55.844102Z",
          "shell.execute_reply.started": "2023-09-02T14:57:55.835055Z",
          "shell.execute_reply": "2023-09-02T14:57:55.843084Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "eBOwgEBdM9uB",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:58:04.259986Z",
          "iopub.execute_input": "2023-09-02T14:58:04.260419Z",
          "iopub.status.idle": "2023-09-02T14:58:04.267902Z",
          "shell.execute_reply.started": "2023-09-02T14:58:04.260366Z",
          "shell.execute_reply": "2023-09-02T14:58:04.266800Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    # print('input_lang:', input_lang )\n",
        "    # print('output_lang:', output_lang)\n",
        "    # print('pairs:', pairs)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'rus', True)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "id": "6dZOGjd5M9uE",
        "outputId": "d3dbafc2-544d-4932-ba6a-d68ff8a86a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-09-02T14:58:06.532647Z",
          "iopub.execute_input": "2023-09-02T14:58:06.533015Z",
          "iopub.status.idle": "2023-09-02T14:58:31.093088Z",
          "shell.execute_reply.started": "2023-09-02T14:58:06.532983Z",
          "shell.execute_reply": "2023-09-02T14:58:31.091950Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Reading lines...\nRead 479223 sentence pairs\nTrimmed to 27825 sentence pairs\nCounting words...\nCounted words:\nrus 10060\neng 4272\n['вы прощены .', 'you re forgiven .']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Обучение seq2seq по аналогии с занятием"
      ],
      "metadata": {
        "id": "GPUboI0qsN_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Encoder\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vgtWqznCM9uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "m9vm9QBWM9uI",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:58:37.854624Z",
          "iopub.execute_input": "2023-09-02T14:58:37.854984Z",
          "iopub.status.idle": "2023-09-02T14:58:37.862165Z",
          "shell.execute_reply.started": "2023-09-02T14:58:37.854951Z",
          "shell.execute_reply": "2023-09-02T14:58:37.860980Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Decoder\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FwLTlgSyM9uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "PFbuUL1LM9uL",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:58:41.853029Z",
          "iopub.execute_input": "2023-09-02T14:58:41.853397Z",
          "iopub.status.idle": "2023-09-02T14:58:41.862309Z",
          "shell.execute_reply.started": "2023-09-02T14:58:41.853351Z",
          "shell.execute_reply": "2023-09-02T14:58:41.861264Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# перевод пар в тензоры\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "z6gGPtXFM9uQ",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:58:46.027758Z",
          "iopub.execute_input": "2023-09-02T14:58:46.028536Z",
          "iopub.status.idle": "2023-09-02T14:58:46.040708Z",
          "shell.execute_reply.started": "2023-09-02T14:58:46.028493Z",
          "shell.execute_reply": "2023-09-02T14:58:46.038506Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция обучения на одном предложении\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "8Fn8VDv8M9uS",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:59:00.446728Z",
          "iopub.execute_input": "2023-09-02T14:59:00.447078Z",
          "iopub.status.idle": "2023-09-02T14:59:00.458959Z",
          "shell.execute_reply.started": "2023-09-02T14:59:00.447049Z",
          "shell.execute_reply": "2023-09-02T14:59:00.457998Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "EKsdwPmSM9uU",
        "execution": {
          "iopub.status.busy": "2023-09-02T14:59:09.701489Z",
          "iopub.execute_input": "2023-09-02T14:59:09.701859Z",
          "iopub.status.idle": "2023-09-02T14:59:09.708104Z",
          "shell.execute_reply.started": "2023-09-02T14:59:09.701810Z",
          "shell.execute_reply": "2023-09-02T14:59:09.706980Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция обучения модели\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "    return plot_losses"
      ],
      "metadata": {
        "id": "C_z_k5IiM9uX",
        "execution": {
          "iopub.status.busy": "2023-09-02T15:03:01.190858Z",
          "iopub.execute_input": "2023-09-02T15:03:01.191229Z",
          "iopub.status.idle": "2023-09-02T15:03:19.986676Z",
          "shell.execute_reply.started": "2023-09-02T15:03:01.191194Z",
          "shell.execute_reply": "2023-09-02T15:03:19.985168Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# отрисовка графика лоса\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "0JXG-RzCM9uZ",
        "execution": {
          "iopub.status.busy": "2023-09-02T15:03:19.988918Z",
          "iopub.execute_input": "2023-09-02T15:03:19.989264Z",
          "iopub.status.idle": "2023-09-02T15:03:20.004963Z",
          "shell.execute_reply.started": "2023-09-02T15:03:19.989229Z",
          "shell.execute_reply": "2023-09-02T15:03:20.002978Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# фунцкия для проверки качества на сгенерированном предложении\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "metadata": {
        "id": "3Bxf45h6M9ud",
        "execution": {
          "iopub.status.busy": "2023-09-02T15:03:20.006227Z",
          "iopub.execute_input": "2023-09-02T15:03:20.006591Z",
          "iopub.status.idle": "2023-09-02T15:03:20.028488Z",
          "shell.execute_reply.started": "2023-09-02T15:03:20.006557Z",
          "shell.execute_reply": "2023-09-02T15:03:20.027279Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "1qUmQIGwM9uf",
        "execution": {
          "iopub.status.busy": "2023-09-02T15:03:20.030867Z",
          "iopub.execute_input": "2023-09-02T15:03:20.031356Z",
          "iopub.status.idle": "2023-09-02T15:03:20.046669Z",
          "shell.execute_reply.started": "2023-09-02T15:03:20.031322Z",
          "shell.execute_reply": "2023-09-02T15:03:20.045734Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение и результат"
      ],
      "metadata": {
        "id": "pU_FSGfc5jGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "history = trainIters(encoder1, decoder1, 75000, print_every=5000)"
      ],
      "metadata": {
        "id": "s_56t10oM9uh",
        "outputId": "0a209fd7-ff34-4b63-cb1d-47d11152dd63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-09-02T15:03:20.048259Z",
          "iopub.execute_input": "2023-09-02T15:03:20.048728Z",
          "iopub.status.idle": "2023-09-02T15:16:26.814348Z",
          "shell.execute_reply.started": "2023-09-02T15:03:20.048695Z",
          "shell.execute_reply": "2023-09-02T15:16:26.813320Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "0m 56s (- 13m 7s) (5000 6%) 3.1090\n1m 47s (- 11m 41s) (10000 13%) 2.6504\n2m 39s (- 10m 36s) (15000 20%) 2.3825\n3m 31s (- 9m 40s) (20000 26%) 2.1668\n4m 23s (- 8m 46s) (25000 33%) 2.0170\n5m 14s (- 7m 52s) (30000 40%) 1.8652\n6m 7s (- 6m 59s) (35000 46%) 1.7516\n6m 59s (- 6m 6s) (40000 53%) 1.6682\n7m 50s (- 5m 13s) (45000 60%) 1.5625\n8m 43s (- 4m 21s) (50000 66%) 1.4688\n9m 36s (- 3m 29s) (55000 73%) 1.4237\n10m 28s (- 2m 37s) (60000 80%) 1.3603\n11m 21s (- 1m 44s) (65000 86%) 1.2862\n12m 14s (- 0m 52s) (70000 93%) 1.2454\n13m 6s (- 0m 0s) (75000 100%) 1.1818\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "common_history = {}\n",
        "common_history ['loss'] = {'gru1' : 0, 'gru2' : 0, 'lstm' : 0}\n",
        "common_history['loss']['gru1']  = round(min(history), 4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T15:28:56.937912Z",
          "iopub.execute_input": "2023-09-02T15:28:56.938581Z",
          "iopub.status.idle": "2023-09-02T15:28:56.946726Z",
          "shell.execute_reply.started": "2023-09-02T15:28:56.938545Z",
          "shell.execute_reply": "2023-09-02T15:28:56.945781Z"
        },
        "trusted": true,
        "id": "Flq-C7-jtRi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "metadata": {
        "id": "xEoEylSyM9uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd67bd5e-bdc2-44db-9ed2-68ba936490fe",
        "execution": {
          "iopub.status.busy": "2023-09-02T15:31:42.781985Z",
          "iopub.execute_input": "2023-09-02T15:31:42.782411Z",
          "iopub.status.idle": "2023-09-02T15:31:42.836562Z",
          "shell.execute_reply.started": "2023-09-02T15:31:42.782355Z",
          "shell.execute_reply": "2023-09-02T15:31:42.835528Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "> он дома сеичас .\n= he s at home right now .\n< he s at home now . <EOS>\n\n> он хитрая лиса .\n= he is a sly fox .\n< he s a bit fat . <EOS>\n\n> я этим очень горжусь .\n= i m very proud of it .\n< i m very proud of my . . <EOS>\n\n> ты с друзьями .\n= you re with friends .\n< you re with friends . <EOS>\n\n> я уверен что том будет пьяныи .\n= i m sure tom will be drunk .\n< i m sure tom will be late . <EOS>\n\n> он сидит на скамеике .\n= he s sitting on the bench .\n< he s on his his . <EOS>\n\n> я этим довольна .\n= i m happy with that .\n< i m happy with that . <EOS>\n\n> мы не глухие .\n= we aren t deaf .\n< we re not deaf . <EOS>\n\n> мы идем на север .\n= we re going north .\n< we re going to the . <EOS>\n\n> я съем твои яблоки .\n= i m going to eat your apples .\n< i m going to eat your . . <EOS>\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Добавление еще +1 рекурентного слоя"
      ],
      "metadata": {
        "id": "jp-CKxOcIhLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Encoder\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TcuW_ub30wJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN2, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru1 = nn.GRU(hidden_size, hidden_size)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru1(output, hidden)\n",
        "        output, hidden = self.gru2(output, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "BpPHbaAL0wJs",
        "execution": {
          "iopub.status.busy": "2023-09-02T15:32:09.286619Z",
          "iopub.execute_input": "2023-09-02T15:32:09.287029Z",
          "iopub.status.idle": "2023-09-02T15:32:09.299322Z",
          "shell.execute_reply.started": "2023-09-02T15:32:09.286998Z",
          "shell.execute_reply": "2023-09-02T15:32:09.298125Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Decoder\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2QFAyJ840wJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN2(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN2, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru1 = nn.GRU(hidden_size, hidden_size)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru1(output, hidden)\n",
        "        output, hidden = self.gru2(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "eETAdIDj0wJs",
        "execution": {
          "iopub.status.busy": "2023-09-02T15:32:09.301664Z",
          "iopub.execute_input": "2023-09-02T15:32:09.302551Z",
          "iopub.status.idle": "2023-09-02T15:32:09.313851Z",
          "shell.execute_reply.started": "2023-09-02T15:32:09.302514Z",
          "shell.execute_reply": "2023-09-02T15:32:09.312862Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "3bpheFHD5Va0",
        "execution": {
          "iopub.status.busy": "2023-09-02T15:56:07.256270Z",
          "iopub.execute_input": "2023-09-02T15:56:07.257557Z",
          "iopub.status.idle": "2023-09-02T15:56:07.268168Z",
          "shell.execute_reply.started": "2023-09-02T15:56:07.257509Z",
          "shell.execute_reply": "2023-09-02T15:56:07.267043Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN2(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN2(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "history = trainIters(encoder1, decoder1, 75000, print_every=5000)"
      ],
      "metadata": {
        "outputId": "b67c4085-e9a7-49ee-9159-a4c47f824422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2e3JKb05Va0",
        "execution": {
          "iopub.status.busy": "2023-09-02T15:56:07.271115Z",
          "iopub.execute_input": "2023-09-02T15:56:07.271714Z",
          "iopub.status.idle": "2023-09-02T16:16:06.210494Z",
          "shell.execute_reply.started": "2023-09-02T15:56:07.271674Z",
          "shell.execute_reply": "2023-09-02T16:16:06.209427Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1m 20s (- 18m 45s) (5000 6%) 3.3222\n2m 38s (- 17m 9s) (10000 13%) 2.9352\n3m 57s (- 15m 49s) (15000 20%) 2.7333\n5m 16s (- 14m 31s) (20000 26%) 2.6432\n6m 36s (- 13m 12s) (25000 33%) 2.5248\n7m 56s (- 11m 54s) (30000 40%) 2.4399\n9m 16s (- 10m 35s) (35000 46%) 2.3037\n10m 36s (- 9m 17s) (40000 53%) 2.1720\n11m 56s (- 7m 57s) (45000 60%) 2.0961\n13m 17s (- 6m 38s) (50000 66%) 1.9707\n14m 36s (- 5m 18s) (55000 73%) 1.8957\n15m 57s (- 3m 59s) (60000 80%) 1.8225\n17m 18s (- 2m 39s) (65000 86%) 1.7233\n18m 38s (- 1m 19s) (70000 93%) 1.6561\n19m 58s (- 0m 0s) (75000 100%) 1.5926\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "common_history['loss']['gru2']  = round(min(history), 4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T16:16:06.215493Z",
          "iopub.execute_input": "2023-09-02T16:16:06.217864Z",
          "iopub.status.idle": "2023-09-02T16:16:06.227552Z",
          "shell.execute_reply.started": "2023-09-02T16:16:06.217825Z",
          "shell.execute_reply": "2023-09-02T16:16:06.226371Z"
        },
        "trusted": true,
        "id": "IJMEBDZHtRi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKaeq5FO5cmF",
        "outputId": "f60c7934-a55e-4563-959d-76d55743f405",
        "execution": {
          "iopub.status.busy": "2023-09-02T16:16:06.232448Z",
          "iopub.execute_input": "2023-09-02T16:16:06.235349Z",
          "iopub.status.idle": "2023-09-02T16:16:06.374625Z",
          "shell.execute_reply.started": "2023-09-02T16:16:06.235300Z",
          "shell.execute_reply": "2023-09-02T16:16:06.372851Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "> я горжусь своеи дочерью .\n= i m proud of my daughter .\n< i m proud of my cat . <EOS>\n\n> я рада что не сделала этого .\n= i m glad i didn t do that .\n< i m glad i didn t do that . <EOS>\n\n> я уверен что том будет здесь к .\n= i m sure tom will be here by .\n< i m sure tom will be here . <EOS>\n\n> сеичас они в одиночестве .\n= they re now alone .\n< they re now alone now . <EOS>\n\n> надеюсь том скоро позвонит .\n= i m hoping tom will call soon .\n< i m hoping tom tom tom tom . <EOS>\n\n> я не собираюсь этого делать .\n= i m not going to do this .\n< i m not going to do that yet . <EOS>\n\n> ты невероятно тупои .\n= you re incredibly stupid .\n< you re incredibly stupid . <EOS>\n\n> я не закончил с тобои разговаривать .\n= i m not finished talking to you .\n< i m not finished talking to you . <EOS>\n\n> теперь я женат .\n= i m married now .\n< i m married now . <EOS>\n\n> наш разговор окончен .\n= we re done talking .\n< we re out of time . <EOS>\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Замена GRU на LSTM"
      ],
      "metadata": {
        "id": "NN-MdDzAvSNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Encoder\n"
      ],
      "metadata": {
        "id": "_SNu_jnAOLoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, hidden_size)\n",
        "        # self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.lstm(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "b4fbDIveCQi8",
        "execution": {
          "iopub.status.busy": "2023-09-02T16:16:06.385373Z",
          "iopub.execute_input": "2023-09-02T16:16:06.387961Z",
          "iopub.status.idle": "2023-09-02T16:16:06.411367Z",
          "shell.execute_reply.started": "2023-09-02T16:16:06.387920Z",
          "shell.execute_reply": "2023-09-02T16:16:06.407515Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Decoder\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hovhMGuiCQi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden  = self.lstm(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "3uSC9al7CQi8",
        "execution": {
          "iopub.status.busy": "2023-09-02T16:16:06.412988Z",
          "iopub.execute_input": "2023-09-02T16:16:06.414133Z",
          "iopub.status.idle": "2023-09-02T16:16:06.436434Z",
          "shell.execute_reply.started": "2023-09-02T16:16:06.414088Z",
          "shell.execute_reply": "2023-09-02T16:16:06.434353Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "gggBV9WFDrDG",
        "execution": {
          "iopub.status.busy": "2023-09-02T16:16:06.444087Z",
          "iopub.execute_input": "2023-09-02T16:16:06.446417Z",
          "iopub.status.idle": "2023-09-02T16:16:06.463457Z",
          "shell.execute_reply.started": "2023-09-02T16:16:06.446365Z",
          "shell.execute_reply": "2023-09-02T16:16:06.462501Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderLSTM(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderLSTM(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "history = trainIters(encoder1, decoder1, 75000, print_every=5000)"
      ],
      "metadata": {
        "outputId": "0a874c5a-15c5-4cc5-f8e2-4c3b25f074d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PG8w_6dDrDG",
        "execution": {
          "iopub.status.busy": "2023-09-02T16:16:06.468837Z",
          "iopub.execute_input": "2023-09-02T16:16:06.471246Z",
          "iopub.status.idle": "2023-09-02T23:57:19.563039Z",
          "shell.execute_reply.started": "2023-09-02T16:16:06.471209Z",
          "shell.execute_reply": "2023-09-02T23:57:19.561597Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "29m 54s (- 418m 47s) (5000 6%) 3.8168\n61m 0s (- 396m 32s) (10000 13%) 3.5143\n91m 59s (- 367m 59s) (15000 20%) 3.4560\n123m 6s (- 338m 32s) (20000 26%) 3.4669\n153m 58s (- 307m 57s) (25000 33%) 3.4285\n184m 58s (- 277m 27s) (30000 40%) 3.4389\n216m 2s (- 246m 54s) (35000 46%) 3.4211\n246m 48s (- 215m 57s) (40000 53%) 3.3804\n277m 32s (- 185m 1s) (45000 60%) 3.4153\n308m 9s (- 154m 4s) (50000 66%) 3.3843\n338m 51s (- 123m 13s) (55000 73%) 3.3715\n369m 21s (- 92m 20s) (60000 80%) 3.3579\n400m 8s (- 61m 33s) (65000 86%) 3.3906\n430m 41s (- 30m 45s) (70000 93%) 3.3613\n461m 9s (- 0m 0s) (75000 100%) 3.3639\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "common_history['loss']['lstm']  = round(min(history), 4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T23:57:19.564539Z",
          "iopub.execute_input": "2023-09-02T23:57:19.564980Z",
          "iopub.status.idle": "2023-09-02T23:57:19.574706Z",
          "shell.execute_reply.started": "2023-09-02T23:57:19.564921Z",
          "shell.execute_reply": "2023-09-02T23:57:19.573538Z"
        },
        "trusted": true,
        "id": "-LUI3S22tRi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-02T23:57:19.575950Z",
          "iopub.execute_input": "2023-09-02T23:57:19.576658Z",
          "iopub.status.idle": "2023-09-02T23:57:20.894551Z",
          "shell.execute_reply.started": "2023-09-02T23:57:19.576624Z",
          "shell.execute_reply": "2023-09-02T23:57:20.893536Z"
        },
        "trusted": true,
        "id": "LfgGUPWYtRi4",
        "outputId": "39bfef7f-9c47-4c84-baf2-37ceb0e48582"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "> я слесарь .\n= i m a locksmith .\n< i m not . . <EOS>\n\n> я поздно спохватился .\n= i m too late .\n< i m not . . <EOS>\n\n> я не обижена .\n= i m not offended .\n< i m not . . <EOS>\n\n> вы забавная .\n= you re funny .\n< i m not . . <EOS>\n\n> вы незаменимы .\n= you aren t replaceable .\n< i m not . . <EOS>\n\n> здесь ты в безопасности .\n= you re safe here .\n< i m not . . <EOS>\n\n> я боюсь волков .\n= i m afraid of wolves .\n< i m not . . <EOS>\n\n> ты настоящии друг .\n= you re a real friend .\n< i m not . . <EOS>\n\n> я так пить хочу .\n= i m so thirsty .\n< i m not . . <EOS>\n\n> я это есть не буду .\n= i m not eating this .\n< i m not . . <EOS>\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Результат сравнения"
      ],
      "metadata": {
        "id": "-IDBMePJTEyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_result = pd.DataFrame.from_dict(common_history)\n",
        "compare_result"
      ],
      "metadata": {
        "id": "6YEqp3ioTLDV",
        "execution": {
          "iopub.status.busy": "2023-09-02T23:57:20.896057Z",
          "iopub.execute_input": "2023-09-02T23:57:20.896440Z",
          "iopub.status.idle": "2023-09-02T23:57:20.908302Z",
          "shell.execute_reply.started": "2023-09-02T23:57:20.896404Z",
          "shell.execute_reply": "2023-09-02T23:57:20.907418Z"
        },
        "trusted": true,
        "outputId": "14c12844-8538-4cf2-af78-cf3f80d94550"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 68,
          "output_type": "execute_result",
          "data": {
            "text/plain": "        loss\ngru1  0.9617\ngru2  1.3585\nlstm  3.0657",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>gru1</th>\n      <td>0.9617</td>\n    </tr>\n    <tr>\n      <th>gru2</th>\n      <td>1.3585</td>\n    </tr>\n    <tr>\n      <th>lstm</th>\n      <td>3.0657</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод: на данном датасете лучший результат получился у модели с одним слоем GRU."
      ],
      "metadata": {
        "id": "4GbEhfiW2mRs"
      }
    }
  ]
}